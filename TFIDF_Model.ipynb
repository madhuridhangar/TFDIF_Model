{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np \n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = [\n",
    "    'This is Term Frequency',\n",
    "    'This is Document Frequency',\n",
    "    'This is Inverse Document Frequency',\n",
    "    'This is Term Frequency - Inverse Document Frequency'            \n",
    "]\n",
    "\n",
    "\n",
    "#text preprocessing\n",
    "\n",
    "stop = set(stopwords.words('english'))#set of stop words\n",
    "sno = nltk.stem.SnowballStemmer('english')#initialising the Snowball Stemmer\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[ ? | ! | \\' | \" | # ] ' , r'' , sentence)\n",
    "    cleaned = re.sub(r'[ . | , | ) | ( | \\ | /]' , r'', cleaned)                     \n",
    "    return cleaned\n",
    "\n",
    "#code for implementing step-by-step the checks mentioned in pre-processing\n",
    "i = 0\n",
    "str1 = ' '\n",
    "final_string =[]\n",
    "s = ''\n",
    "for sent in corpus:\n",
    "    filtered_sentence =[]\n",
    "    #print(sent);\n",
    "    sent = cleanhtml(sent) #remove HTML tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if(cleaned_words.isalpha()):\n",
    "                s = (sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                filtered_sentence.append(s)\n",
    "                        \n",
    "            else:\n",
    "                continue\n",
    "    #print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of clean words\n",
    "    str1 =str1.lower()\n",
    "    final_string.append(str1)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "#print(type(final_string))\n",
    "#print(type(corpus))\n",
    "#print(final_string)\n",
    "#print(corpus)\n",
    "#corpus =list(final_string)\n",
    "#unique_words = set() # at first we will initialize an empty set\n",
    "# check if its list type or not\n",
    "if isinstance(corpus, (list,)):\n",
    "    for row in corpus: # for each review in the dataset\n",
    "        for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            unique_words.add(word)\n",
    "    unique_words = sorted(list(unique_words))\n",
    "#TF\n",
    "def TF(corpus):\n",
    "    tf_dict = {}\n",
    "    corpusCount = len(corpus)\n",
    "    for word, count in corpus.items():\n",
    "        tf_dict[word] = count/float(corpusCount)\n",
    "    return(tf_dict)\n",
    "\n",
    "#IDF\n",
    "def IDF(corpus,unique_words):\n",
    "    idf_dict={}\n",
    "    corpusCount=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "            idf_dict[i]=1+(math.log((1+corpusCount)/(count+1))) #1+log(N/n)\n",
    "            #adding 1 to avoiding divid by zero errors #SCIKIT FORMULA\n",
    "    return idf_dict \n",
    "\n",
    "#TFDIF\n",
    "\n",
    "def TFIDF(tfs,idfs):\n",
    "    tfidf_dict={}\n",
    "    for word, val in tfs.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf_dict)\n",
    "\n",
    "# it accepts only list of sentances\n",
    "    \n",
    "#we need prepocessing text here\n",
    "def fit(dataset):    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    # check if its list type or not\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset: # for each review in the dataset\n",
    "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        Idf_values_of_all_unique_words = IDF(dataset,unique_words)    \n",
    "    else:\n",
    "        print(\"you need to pass list of sentance\")\n",
    "    return vocab\n",
    "\n",
    "\n",
    "Vocabulary=fit(corpus)\n",
    "idf_of_vocabulary=fit(corpus)\n",
    "\n",
    "print('fit')\n",
    "print(idf_of_vocabulary)\n",
    "print('features')\n",
    "print(list(Vocabulary.keys()))     \n",
    "print(list(idf_of_vocabulary.values()))\n",
    "print('idf')\n",
    "idf = IDF(corpus,unique_words)\n",
    "print(idf.values())\n",
    "\n",
    "#we need prepocessing text here\n",
    "\n",
    "def transform(dataset,vocabulary,idf_values):\n",
    "    sparse_matrix= csr_matrix( (len(dataset), len(vocabulary)), dtype=np.float64)\n",
    "    for row  in range(0,len(dataset)):\n",
    "        number_of_words_in_sentence=Counter(dataset[row].split())\n",
    "        for word in dataset[row].split():\n",
    "            if word in  list(vocabulary.keys()):\n",
    "                tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
    "                sparse_matrix[row,vocabulary[word]]=tf_idf_value\n",
    "        #print(\"NORM FORM\\n\",normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False))\n",
    "        output =normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "    return output\n",
    "\n",
    "final_output=transform(corpus,Vocabulary,idf_of_vocabulary)\n",
    "print('shape')\n",
    "print(final_output.shape) \n",
    "\n",
    "## SkLearn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)\n",
    "print('^^^^^^^^^^^^')\n",
    "print('SkLearn')\n",
    "print('features')\n",
    "print(vectorizer.get_feature_names())\n",
    "print('idf')\n",
    "print(vectorizer.idf_)\n",
    "print('shape')\n",
    "skl_output.shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
