{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "\n",
    "corpus = [\n",
    "    'This is Term Frequency',\n",
    "    'This is Document Frequency',\n",
    "    'This is Inverse Document Frequency',\n",
    "    'This is Term Frequency - Inverse Document Frequency'            \n",
    "]\n",
    "\n",
    "#text preprocessing\n",
    "\n",
    "stop = set(stopwords.words('english'))#set of stop words\n",
    "sno = nltk.stem.SnowballStemmer('english')#initialising the Snowball Stemmer\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[ ? | ! | \\' | \" | # ] ' , r'' , sentence)\n",
    "    cleaned = re.sub(r'[ . | , | ) | ( | \\ | /]' , r'', cleaned)                     \n",
    "    return cleaned\n",
    "\n",
    "#code for implementing step-by-step the checks mentioned in pre-processing\n",
    "i = 0\n",
    "str1 = ' '\n",
    "final_string =[]\n",
    "s = ''\n",
    "for sent in corpus:\n",
    "    filtered_sentence =[]\n",
    "    #print(sent);\n",
    "    sent = cleanhtml(sent) #remove HTML tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words) > 2)):\n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s = (sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            else:\n",
    "                continue\n",
    "    #print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of clean words\n",
    "    final_string.append(str1)\n",
    "    i += 1\n",
    "\n",
    "CleanedText = final_string\n",
    "\n",
    "unique_word = {\" \".join (corpus).split()}\n",
    "\n",
    "#TF\n",
    "def TF(corpus, unique_word):\n",
    "    tf_dict = {}\n",
    "    corpusCount = len(corpus)\n",
    "    for word, count in corpus.items():\n",
    "        tf_dict[word] = count/float(corpusCount)\n",
    "    return(tf_dict)\n",
    "\n",
    "#IDF\n",
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    corpusCount=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "            idf_dict[i]=1+(math.log((1+corpusCount)/(count+1))) #1+log(N/n)\n",
    "            #adding 1 to avoiding divid by zero errors #SCIKIT FORMULA\n",
    "    return idf_dict \n",
    "\n",
    "#TFDIF\n",
    "\n",
    "def TFIDF(tfs,idfs):\n",
    "    tfidf_dict={}\n",
    "    for word, val in tfs.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf_dict)\n",
    "    # it accepts only list of sentances\n",
    "    \n",
    "    #we need prepocessing text here\n",
    "    def fit(dataset):    \n",
    "        unique_words = set() # at first we will initialize an empty set\n",
    "        # check if its list type or not\n",
    "        if isinstance(dataset, (list,)):\n",
    "            for row in dataset: # for each review in the dataset\n",
    "                for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "                    if len(word) < 2:\n",
    "                        continue\n",
    "                    unique_words.add(word)\n",
    "            unique_words = sorted(list(unique_words))\n",
    "            vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        \n",
    "            return vocab\n",
    "        else:\n",
    "            print(\"you need to pass list of sentance\")\n",
    "     \n",
    "    #we need prepocessing text here\n",
    "    def transform(dataset,vocab):\n",
    "        rows = []\n",
    "        columns = []\n",
    "        values = []\n",
    "        if isinstance(dataset, (list,)):\n",
    "            for idx, row in enumerate(tqdm(dataset)): # for each document in the dataset\n",
    "                # it will return a dict type object where key is the word and values is its frequency, {word:frequency}\n",
    "                word_freq = dict(Counter(row.split()))\n",
    "                # for every unique word in the document\n",
    "                for word, freq in word_freq.items():  # for each unique word in the review.                \n",
    "                    if len(word) < 2:\n",
    "                        continue\n",
    "                    # we will check if its there in the vocabulary that we build in fit() function\n",
    "                    # dict.get() function will return the values, if the key doesn't exits it will return -1\n",
    "                    col_index = vocab.get(word, -1) # retreving the dimension number of a word\n",
    "                    # if the word exists\n",
    "                    if col_index !=-1:\n",
    "                        # we are storing the index of the document\n",
    "                        rows.append(idx)\n",
    "                        # we are storing the dimensions of the word\n",
    "                        columns.append(col_index)\n",
    "                        # we are storing the frequency of the word\n",
    "                        values.append(freq)\n",
    "            return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "        else:\n",
    "            print(\"you need to pass list of strings\")  \n",
    "    \n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
